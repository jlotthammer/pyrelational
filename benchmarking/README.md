# Benchmarking overview

The benchmark results are located within a subdirectory for each dataset in `results.csv`. These results are generated by a python script in the same folder, currently named `run.py`. Each run script is styled in a similar pattern containing a `trial` function which specifies the manner in which the PyRelationAL pipeline is constructed. Done so as we may want different model_managers, oracles, seeds, etc. to be run. It also defines settings for names, and resources to be used for the trials.

For each experiment, the DataManager is defined in a separate script, specifying the train, val, test splits along with the initial labelled and unlabelled indices in the queryable (train) pool.

In current benchmarks, we leverage [Ray Tune's job scheduling](https://docs.ray.io/en/latest/tune/index.html) to efficiently distribute and manage the running of the jobs across available hardware. Each job creates an individual "trial" with its results and other logs sent to the `results.csv`. After this, we can collate and analyze the trials at the trial or whole experiment level by reading these results files in the `visualisation.ipynb` notebook.

# TL;DR

To re-run a benchmark experiment with the same selection of strategies as in the paper: run the following command from the project repository
```bash
python -m benchmarking.<dataset>.run
```

- If adding a strategy to an existing benchmark: adjust `run.py`, then run it with `python -m benchmarking/<dataset>/run`.
- If adding a new dataset: add a `<dataset>` folder under `benchmarking/` then add the necessary scripts underneath following examples on the repository. Most important in this case is the `data_manager.py` which will specify the train, validation, test, initial labelled, and initial unlabelled indices for the dataset.


# Utilities
We provide some genetic benchmarking utils in `benchmarking_utils.py` along with classification and regression specific utilities and model definitions in `classification_experiment_utils.py` and `regression_experiment_utils.py` respectively.

- `benchmarking_utils.py`: Contains general utilities for processing the outputs of the ray benchmarks.
- `classification_experiment_utils.py`: Provides utilities and model definitions specific to classification tasks, including data preprocessing, model training, and evaluation functions. It also contains utilities for quickly calling classification specific AL strategies and parameter spaces for the experiments. It may be useful to add to these when trying new strategies.
- `regression_experiment_utils.py`: Offers utilities and model definitions tailored for regression tasks, covering data handling, model training, and performance evaluation. It also contains utilities for quickly calling regression specific AL strategies and parameter spaces for the experiments. It may be useful to add to these when trying new strategies.
